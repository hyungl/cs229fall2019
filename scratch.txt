Project notes:
11/15/2019
Step1:
Split google speech commands v0.02 into train/validation and test set using all wav files and validation_list and testing_list
that includes the files to be used for validation/test purpose
Train set has 169700 wav files
Validation set has 9981 wav files
Test set has 11005 wav files

Min. size of file that do not contain noise is 7k
Max. size of file that do not contain noise is 32k
Max. size of file that contain noise is 3M

32kB files correspond to time series of length 44.
Therefore we set our maximum time step to 44 and truncate noise files to up to this time stamp.


Things to try:
Hyperparameter tuning: # of mfcc components, # of components in HMM model, number of gaussians in GMM, currently noise is truncated
instead if pad signal with zeros or append to wrap over from time0 would that improve model performance.
Elaborate on error anayisis, add visualizations

Other models to try: CNN, LSTM/RNN?

Other ML techniques to try.
Clustering, k fold validation




Reference material:
```
@article{speechcommandsv2,
   author = {{Warden}, P.},
    title = "{Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1804.03209},
 primaryClass = "cs.CL",
 keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
     year = 2018,
    month = apr,
    url = {https://arxiv.org/abs/1804.03209},
}
```

# Collection

The audio files were collected using crowdsourcing. The goal was to gather examples of
people speaking single-word commands, rather than conversational sentences, so
they were prompted for individual words over the course of a five minute
session. Twenty core command words were recorded, with most speakers saying each
of them five times. The core words are "Yes", "No", "Up", "Down", "Left",
"Right", "On", "Off", "Stop", "Go", "Zero", "One", "Two", "Three", "Four",
"Five", "Six", "Seven", "Eight", and "Nine". To help distinguish unrecognized
words, there are also ten auxiliary words, which most speakers only said once.
These include "Bed", "Bird", "Cat", "Dog", "Happy", "House", "Marvin", "Sheila",
"Tree", and "Wow".

## Organization

The files are organized into folders, with each directory name labelling the
word that is spoken in all the contained audio files. No details were kept of
any of the participants age, gender, or location, and random ids were assigned
to each individual. These ids are stable though, and encoded in each file name
as the first part before the underscore. If a participant contributed multiple
utterances of the same word, these are distinguished by the number at the end of
the file name. For example, the file path `happy/3cfc6b3a_nohash_2.wav`
indicates that the word spoken was "happy", the speaker's id was "3cfc6b3a", and
this is the third utterance of that word by this speaker in the data set. The
'nohash' section is to ensure that all the utterances by a single speaker are
sorted into the same training partition, to keep very similar repetitions from
giving unrealistically optimistic evaluation scores.

## Partitioning

The audio clips haven't been separated into training, test, and validation sets
explicitly, but by convention a hashing function is used to stably assign each
file to a set. Here's some Python code demonstrating how a complete file path
and the desired validation and test set sizes (usually both 10%) are used to
assign a set:
